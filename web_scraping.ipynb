{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70cc61ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row      dr_no            date_rptd             date_occ time_occ area  \\\n",
      "0  NaN        NaN                  NaN                  NaN      NaN  NaN   \n",
      "1  NaN  010304468  2020-01-08T00:00:00  2020-01-08T00:00:00     2230   03   \n",
      "2  NaN  190101086  2020-01-02T00:00:00  2020-01-01T00:00:00     0330   01   \n",
      "3  NaN  200110444  2020-04-14T00:00:00  2020-02-13T00:00:00     1200   01   \n",
      "4  NaN  191501505  2020-01-01T00:00:00  2020-01-01T00:00:00     1730   15   \n",
      "\n",
      "     area_name rpt_dist_no part_1_2 crm_cd  ... status   status_desc crm_cd_1  \\\n",
      "0          NaN         NaN      NaN    NaN  ...    NaN           NaN      NaN   \n",
      "1    Southwest        0377        2    624  ...     AO   Adult Other      624   \n",
      "2      Central        0163        2    624  ...     IC   Invest Cont      624   \n",
      "3      Central        0155        2    845  ...     AA  Adult Arrest      845   \n",
      "4  N Hollywood        1543        2    745  ...     IC   Invest Cont      745   \n",
      "\n",
      "                                  location      lat        lon crm_cd_2  \\\n",
      "0                                      NaN      NaN        NaN      NaN   \n",
      "1  1100 W  39TH                         PL  34.0141  -118.2978      NaN   \n",
      "2   700 S  HILL                         ST  34.0459  -118.2545      NaN   \n",
      "3   200 E  6TH                          ST  34.0448  -118.2474      NaN   \n",
      "4  5400    CORTEEN                      PL  34.1685  -118.4019      998   \n",
      "\n",
      "  cross_street crm_cd_3 crm_cd_4  \n",
      "0          NaN      NaN      NaN  \n",
      "1          NaN      NaN      NaN  \n",
      "2          NaN      NaN      NaN  \n",
      "3          NaN      NaN      NaN  \n",
      "4          NaN      NaN      NaN  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "class XMLDataParser:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "    def fetch_xml(self):\n",
    "        response = requests.get(self.url)\n",
    "        response.raise_for_status()  # Raises an HTTPError if the HTTP request returned an unsuccessful status code\n",
    "        return response.content\n",
    "\n",
    "    def parse_xml_to_df(self, xml_data):\n",
    "        root = ET.fromstring(xml_data)\n",
    "        all_records = []\n",
    "        # Assuming that each 'row' element in the XML contains the data record\n",
    "        for row in root.findall('.//row'):\n",
    "            record = {}\n",
    "            for child in row:\n",
    "                # Create a dictionary item with the 'row' tag names as keys and text content as values\n",
    "                record[child.tag] = child.text\n",
    "            all_records.append(record)\n",
    "        return pd.DataFrame(all_records)\n",
    "\n",
    "# Usage\n",
    "url = 'https://data.lacity.org/api/views/2nrs-mtv8/rows.xml?accessType=DOWNLOAD'\n",
    "parser = XMLDataParser(url)\n",
    "xml_content = parser.fetch_xml()\n",
    "df = parser.parse_xml_to_df(xml_content)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7663a198",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1057754415.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install pandas requests xml.etree.ElementTree\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install pandas requests xml.etree.ElementTree\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9424c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "class ConsumerComplaintsAPIClient:\n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url\n",
    "\n",
    "    def get_data(self, params={}):\n",
    "        response = requests.get(self.base_url, params=params)\n",
    "        response.raise_for_status()  # Raises an HTTPError if the status is 4xx or 5xx\n",
    "        return response.json()\n",
    "\n",
    "    def to_dataframe(self, json_data):\n",
    "        # Adjust this if the JSON structure is different\n",
    "        return pd.DataFrame(json_data.get('hits', {}).get('hits', []))\n",
    "\n",
    "# Usage example:\n",
    "base_url = 'https://www.consumerfinance.gov/data-research/consumer-complaints/search/api/v1/'\n",
    "client = ConsumerComplaintsAPIClient(base_url)\n",
    "\n",
    "# Example params - customize these as needed based on the API's documentation\n",
    "params = {\n",
    "    'size': 10  # Limits the number of results returned\n",
    "}\n",
    "\n",
    "json_response = client.get_data(params=params)\n",
    "# The API nests the records under 'hits' -> 'hits', and each actual record is under the '_source' key\n",
    "df = client.to_dataframe(json_response)\n",
    "\n",
    "# Since each complaint is nested under the '_source' key, we extract this into a separate DataFrame\n",
    "df = pd.json_normalize(df['_source'])\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c4e24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests beautifulsoup4 pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abdf166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL of the webpage with the table\n",
    "url = 'https://gaspgroup.org/air-quality/?gad_source=1&gclid=Cj0KCQiAmNeqBhD4ARIsADsYfTdQN2SF83cBgN2EFT0xmngSf21-WoH8fTOfuHobIivaQlaNAyodeH8aAs8tEALw_wcB'\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Ensure we notice bad responses\n",
    "\n",
    "# Parse the HTML content of the page with BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the table in the soup object - you would need to identify the correct table by an id or class if there are multiple tables\n",
    "table = soup.find('table')  # You may need to adjust this if the table has a specific class or id\n",
    "\n",
    "# Extract the table rows\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "# The first row usually contains the header columns\n",
    "headers = [header.get_text().strip() for header in rows[0].find_all('th')]\n",
    "\n",
    "# Extract the data from the table into a list of dictionaries\n",
    "table_data = []\n",
    "for row in rows[1:]:  # Skip the header row\n",
    "    cells = row.find_all('td')\n",
    "    cell_data = {headers[i]: cell.get_text().strip() for i, cell in enumerate(cells)}\n",
    "    table_data.append(cell_data)\n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "df = pd.DataFrame(table_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a289035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Configure ChromeOptions to run headless if you don't need a browser UI\n",
    "options = Options()\n",
    "options.headless = True\n",
    "\n",
    "# Set up the Chrome WebDriver\n",
    "# Make sure you have downloaded the correct version of 'chromedriver' for your Chrome version and placed it in your PATH or specified location.\n",
    "service = Service(executable_path='path/to/chromedriver')\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Open the page\n",
    "driver.get('https://gaspgroup.org/air-quality/?gad_source=1&gclid=Cj0KCQiAmNeqBhD4ARIsADsYfTdQN2SF83cBgN2EFT0xmngSf21-WoH8fTOfuHobIivaQlaNAyodeH8aAs8tEALw_wcB')\n",
    "\n",
    "# Wait for JavaScript to load. This time might need to be adjusted.\n",
    "time.sleep(5)\n",
    "\n",
    "# Now that the page is fully loaded, grab the HTML content\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Assuming the table has a unique identifier, find that table\n",
    "# If the table doesn't have an id or class, you would need to find another way to locate it\n",
    "table = soup.find('table', {'id': 'unique_table_id'})  # Replace with the actual id or class\n",
    "\n",
    "# Extract the rows from the table, assuming the table is well structured with <tr> and <td> tags\n",
    "rows = table.find_all('tr') if table else []\n",
    "\n",
    "# Proceed with data extraction as before\n",
    "# ...\n",
    "\n",
    "# Don't forget to close the driver after you're done\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bde51289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Obtaining dependency information for selenium from https://files.pythonhosted.org/packages/0e/59/aae37fa93e2d4292c3148efcc3066c8ecfe5cfaa72bf8c0b1a5614622cf7/selenium-4.15.2-py3-none-any.whl.metadata\n",
      "  Downloading selenium-4.15.2-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /Users/Shared/anaconda3/lib/python3.11/site-packages (from selenium) (1.26.16)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Obtaining dependency information for trio~=0.17 from https://files.pythonhosted.org/packages/39/46/620fbe56f41fa3ccdda2136d947fb9bacce3d1eb163f057f0262a0ddf5e0/trio-0.23.1-py3-none-any.whl.metadata\n",
      "  Downloading trio-0.23.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Obtaining dependency information for trio-websocket~=0.9 from https://files.pythonhosted.org/packages/48/be/a9ae5f50cad5b6f85bd2574c2c923730098530096e170c1ce7452394d7aa/trio_websocket-0.11.1-py3-none-any.whl.metadata\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /Users/Shared/anaconda3/lib/python3.11/site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=20.1.0 in /Users/Shared/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: sortedcontainers in /Users/Shared/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /Users/Shared/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Obtaining dependency information for outcome from https://files.pythonhosted.org/packages/55/8b/5ab7257531a5d830fc8000c476e63c935488d74609b50f9384a643ec0a62/outcome-1.3.0.post0-py2.py3-none-any.whl.metadata\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio~=0.17->selenium)\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /Users/Shared/anaconda3/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading selenium-4.15.2-py3-none-any.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading trio-0.23.1-py3-none-any.whl (448 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m448.3/448.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: sniffio, outcome, h11, wsproto, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: sniffio\n",
      "    Found existing installation: sniffio 1.2.0\n",
      "    Uninstalling sniffio-1.2.0:\n",
      "      Successfully uninstalled sniffio-1.2.0\n",
      "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.15.2 sniffio-1.3.0 trio-0.23.1 trio-websocket-0.11.1 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e45aa6a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m table \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Assuming the first row is the header\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m headers \u001b[38;5;241m=\u001b[39m [th\u001b[38;5;241m.\u001b[39mget_text(strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m th \u001b[38;5;129;01min\u001b[39;00m table\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mth\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Extract the table rows, skipping the header\u001b[39;00m\n\u001b[1;32m     26\u001b[0m rows \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# The URL of the page you want to scrape\n",
    "url = 'https://gaspgroup.org/air-quality/'\n",
    "\n",
    "# Perform the HTTP request to get the webpage content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Raise an exception if the request failed\n",
    "response.raise_for_status()\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the table you want to scrape\n",
    "# You'll need to inspect the webpage to determine the correct identifier for the table\n",
    "# I'll use a placeholder 'table_id' here, but you should replace it with the actual id or class\n",
    "table = soup.find('table', id='table_id')\n",
    "\n",
    "# Assuming the first row is the header\n",
    "headers = [th.get_text(strip=True) for th in table.find_all('th')]\n",
    "\n",
    "# Extract the table rows, skipping the header\n",
    "rows = table.find_all('tr')[1:]\n",
    "\n",
    "# Extract the table data\n",
    "table_data = []\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    row_data = [ele.get_text(strip=True) for ele in cols]\n",
    "    table_data.append(row_data)\n",
    "\n",
    "# Create the DataFrame using the header and rows\n",
    "df = pd.DataFrame(table_data, columns=headers)\n",
    "\n",
    "# Now you have a DataFrame `df` that you can use for analysis\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dc3c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# The URL of the page you want to scrape\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_largest_companies_in_the_United_States_by_revenue'\n",
    "\n",
    "# Perform the HTTP request to get the webpage content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Raise an exception if the request failed\n",
    "response.raise_for_status()\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the table you want to scrape, Wikipedia tables usually have the 'wikitable' class\n",
    "table = soup.find('table', {'class': 'wikitable'})\n",
    "\n",
    "# Check if a table is found\n",
    "if table:\n",
    "    # Extract the header names\n",
    "    headers = [header.get_text(strip=True) for header in table.find_all('th')]\n",
    "\n",
    "    # Extract the table rows, skipping the header\n",
    "    rows = table.find_all('tr')[1:]\n",
    "\n",
    "    # Extract the table data\n",
    "    table_data = []\n",
    "    for row in rows:\n",
    "        cols = row.find_all(['td', 'th'])  # This gets all table data and header cells\n",
    "        row_data = [ele.get_text(strip=True) for ele in cols]\n",
    "        table_data.append(row_data)\n",
    "\n",
    "    # Create the DataFrame using the header and rows\n",
    "    df = pd.DataFrame(table_data, columns=headers)\n",
    "\n",
    "    # Now you have a DataFrame `df` that you can use for analysis\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"No wikitable found on the page.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30a20c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
